% This file was created with Citavi 6.5.0.0

@misc{.,
 title = {A Comparison of Algorithms for Multi-Player Games},
 url = {https://webdocs.cs.ualberta.ca/~nathanst/papers/comparison_algorithms.pdf}
}


@misc{.1082020,
 year = {10/8/2020},
 title = {A hierarchical strategy for path planning among moving obstacles (mobile robot) - IEEE Journals {\&} Magazine},
 urldate = {10/8/2020},
 file = {https://ieeexplore.ieee.org/abstract/document/88018}
}


@misc{.1082020b,
 year = {10/8/2020},
 title = {Evolutionary artificial potential fields and their application in real time robot path planning - IEEE Conference Publication},
 urldate = {10/8/2020},
 file = {https://ieeexplore.ieee.org/abstract/document/870304}
}


@misc{.1082020c,
 year = {10/8/2020},
 title = {Neural-Network-Based Path Planning for a Multirobot System With Moving Obstacles - IEEE Journals {\&} Magazine},
 urldate = {10/8/2020},
 file = {https://ieeexplore.ieee.org/abstract/document/4926151?casa_token=N9roZmXepDcAAAAA:oTlE1DmhzWN4sKQxtJiSWNqerk4_AeLKd6FnMBN84hyh6J4rsSwNzKaOvtMWSDvO2TGSuLE7sA}
}


@misc{.14.01.2021,
 year = {14.01.2021},
 title = {IEEE Xplore Full-Text PDF},
 url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=297950},
 urldate = {14.01.2021}
}


@book{.1986,
 author = {Luckhardt, Carol and  Irani, Keki},
 year = {1986},
 title = {An Algorithmic Solution of N-Person Games},
 url = {https://www.aaai.org/papers/aaai/1986/aaai86-025.pdf},
 file = {An Algorithmic Solution of N-Person 1986.pdf}
}


@proceedings{.2011,
 year = {2011},
 title = {2011 IEEE International Conference on Robotics and Automation}
}


@proceedings{.2015,
 year = {2015},
 title = {Proceedings of the 14th Python in Science Conference},
 publisher = {SciPy},
 series = {Proceedings of the Python in Science Conference}
}


@proceedings{.2019,
 year = {2019},
 title = {11th International Conference on Advanced Computing: 18-20 December 2019 : Department of Computer Technology, Anna University, MIT campus, Chennai},
 address = {[Piscataway, NJ]},
 publisher = {IEEE},
 isbn = {978-1-7281-5286-8},
 doi = {10.1109/ICoAC48765.2019}
}


@book{.b,
 title = {AI 2019: Advances in Aritficial Intelligence},
 file = {38203e35-1d7c-4629-b0ec-0b2cdffa7273:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\38203e35-1d7c-4629-b0ec-0b2cdffa7273.pdf:pdf}
}


@misc{.c,
 title = {Google AI Challenge post-mortem}
}


@article{.d,
 title = {Mastering AchtungDieKurve with Deep Q-Learning using OpenAI Gym},
 file = {691fff5c-9c87-4030-b6ef-b91b59712096:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\691fff5c-9c87-4030-b6ef-b91b59712096.pdf:pdf}
}


@misc{.e,
 title = {N-Person Minimax and Alpha-Beta Pruning},
 url = {https://www.diva-portal.org/smash/get/diva2:761634/FULLTEXT01.pdf},
 file = {65dae0ab-f016-4a0a-9c13-0384bf33593f:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Remote Attachments\\65dae0ab-f016-4a0a-9c13-0384bf33593f.pdf:pdf}
}


@article{.f,
 title = {netlogo-swarmfest2004},
 file = {daf72316-a3e3-457c-b5a3-ba43cf4246c6:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\daf72316-a3e3-457c-b5a3-ba43cf4246c6.pdf:pdf}
}


@misc{.g,
 abstract = {Die Spieltheorie ist eine mathematische Theorie, in der Entscheidungssituationen modelliert werden, in denen mehrere Beteiligte miteinander interagieren. Sie versucht dabei unter anderem, das rationale Entscheidungsverhalten in sozialen Konfliktsituationen davon abzuleiten. Die Spieltheorie ist origin{\"a}r ein Teilgebiet der Mathematik. Sie bedient mannigfaltige Anwendungsfelder.

In diesem Artikel wird die nicht-kooperative Spieltheorie behandelt, die von der kooperativen Spieltheorie zu unterscheiden ist. Unten finden sich einige Bemerkungen zu den Unterschieden.},
 title = {Spieltheorie},
 url = {https://de.wikipedia.org/w/index.php?title=Spieltheorie&oldid=201853908},
 urldate = {2021-01-03}
}


@misc{AndySloane.2010,
 author = {{Andy Sloane}},
 year = {2010},
 title = {Google AI Challenge post-mortem},
 url = {https://www.a1k0n.net/2010/03/04/google-ai-postmortem.html},
 urldate = {14.01.2021}
}


@article{Aurenhammer.1991,
 author = {Aurenhammer, Franz},
 year = {1991},
 title = {Voronoi diagrams---a survey of a fundamental geometric data structure},
 pages = {345--405},
 volume = {23},
 number = {3},
 issn = {0360-0300},
 journal = {ACM Computing Surveys},
 doi = {10.1145/116873.116880},
 file = {23f4a274-9bbd-4f92-9831-08760b8901c0:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\23f4a274-9bbd-4f92-9831-08760b8901c0.pdf:pdf}
}


@misc{Bai.22.06.2020,
 abstract = {This paper considers the problem of designing optimal algorithms for reinforcement learning in two-player zero-sum games. We focus on self-play algorithms which learn the optimal policy by playing against itself without any direct supervision. In a tabular episodic Markov game with $S$ states, $A$ max-player actions and $B$ min-player actions, the best existing algorithm for finding an approximate Nash equilibrium requires {\$}\tilde{\mathcal{O}}(S{\^{}}2AB){\$} steps of game playing, when only highlighting the dependency on {\$}(S,A,B){\$}. In contrast, the best existing lower bound scales as {\$}$\backslash$Omega(S(A+B)){\$} and has a significant gap from the upper bound. This paper closes this gap for the first time: we propose an optimistic variant of the \emph{Nash Q-learning} algorithm with sample complexity {\$}\tilde{\mathcal{O}}(SAB){\$}, and a new \emph{Nash V-learning} algorithm with sample complexity {\$}\tilde{\mathcal{O}}(S(A+B)){\$}. The latter result matches the information-theoretic lower bound in all problem-dependent parameters except for a polynomial factor of the length of each episode. In addition, we present a computational hardness result for learning the best responses against a fixed opponent in Markov games---a learning objective different from finding the Nash equilibrium.},
 author = {Bai, Yu and Jin, Chi and Yu, Tiancheng},
 date = {22.06.2020},
 title = {Near-Optimal Reinforcement Learning with Self-Play},
 url = {http://arxiv.org/pdf/2006.12007v2},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Machine Learning},
 file = {54eb03df-74b0-4467-80fa-d06535af80a8:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\54eb03df-74b0-4467-80fa-d06535af80a8.pdf:pdf}
}


@article{Bernhardt.2007,
 author = {Bernhardt, Ken},
 year = {2007},
 title = {Agent-based modeling in transportation.},
 pages = {72--80},
 journal = {Artificial Intelligence in Transportation},
 file = {e0c3ff46-407e-485a-9cee-ebfd1f573efc:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\e0c3ff46-407e-485a-9cee-ebfd1f573efc.pdf:pdf}
}


@book{Birukou.2019,
 year = {2019},
 title = {AI 2019: Advances in Artificial Intelligence},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-35287-5},
 series = {Lecture Notes in Computer Science},
 editor = {Birukou and Liu},
 doi = {10.1007/978-3-030-35288-2}
}


@article{Bodlaender.2001,
 abstract = {1990-11},
 author = {Bodlaender, H. and Kloks, T.},
 year = {2001},
 title = {Fast Algorithms for the TRON Game on Trees},
 file = {5c5bfcac-22c8-46a7-9199-7e51b5d532da:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\5c5bfcac-22c8-46a7-9199-7e51b5d532da.pdf:pdf}
}


@article{Borenstein.,
 abstract = {A real-time obstacle avoidance method for mobile robots which has been developed and implemented is described. This method, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target. The VFH method uses a two-dimensional Cartesian histogram grid as a world model. This world model is updated continuously with range data sampled by onboard range sensors. The VFH method subsequently uses a two-stage data-reduction process to compute the desired control commands for the vehicle. Experimental results from a mobile robot traversing densely cluttered obstacle courses in smooth and continuous motion and at an average speed of 0.6-0.7 m/s are shown. A comparison of the VFN method to earlier methods is given.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
 author = {Borenstein, J. and Koren, Y.},
 title = {The vector field histogram-fast obstacle avoidance for mobile robots},
 pages = {278--288},
 volume = {7},
 number = {3},
 issn = {1042-296X},
 journal = {1042-296X},
 doi = {10.1109/70.88137}
}


@misc{Brockman.2016,
 abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
 author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
 date = {2016},
 title = {OpenAI Gym},
 url = {https://arxiv.org/pdf/1606.01540},
 keywords = {Artificial Intelligence (cs.AI);Machine Learning (cs.LG)},
 file = {https://arxiv.org/pdf/1606.01540.pdf}
}


@article{Campbell.1983,
 author = {Campbell, Murray S. and Marsland, T. Anthony},
 year = {1983},
 title = {A comparison of minimax tree search algorithms},
 pages = {347--367},
 volume = {20},
 number = {4},
 issn = {0004-3702},
 journal = {Artificial Intelligence}
}


@article{Campbell.2002,
 abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database.

This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
 author = {Campbell, Murray and Hoane, A.Joseph and Hsu, Feng-hsiung},
 year = {2002},
 title = {Deep Blue},
 url = {http://www.sciencedirect.com/science/article/pii/S0004370201001291},
 pages = {57--83},
 volume = {134},
 number = {1-2},
 issn = {0004-3702},
 journal = {Artificial Intelligence},
 doi = {10.1016/S0004-3702(01)00129-1},
 file = {d48a749f-15cb-435c-8048-bd45cadaa40d:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\d48a749f-15cb-435c-8048-bd45cadaa40d.pdf:pdf}
}


@article{Gomboc.2005,
 author = {Gomboc, D. and Buro, M. and Marsland, T. A.},
 year = {2005},
 title = {Tuning evaluation functions by maximizing concordance},
 pages = {202--229},
 volume = {349},
 number = {2},
 issn = {03043975},
 journal = {Theoretical Computer Science},
 doi = {10.1016/j.tcs.2005.09.047}
}


@article{Hoske.2015,
 author = {Hoske, Daniel and Rollin, Jonathan and Ueckerdt, Torsten and Walzer, Stefan},
 year = {2015},
 title = {Playing weighted Tron on trees},
 pages = {2341--2347},
 volume = {338},
 number = {12},
 issn = {0012365X},
 journal = {Discrete Mathematics},
 doi = {10.1016/j.disc.2015.06.002},
 file = {3bd01c8a-d8bc-456f-b544-6d2af282358a:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\3bd01c8a-d8bc-456f-b544-6d2af282358a.pdf:pdf}
}


@book{Hutchison.2006,
 year = {2006},
 title = {Computers and Games},
 address = {Berlin, Heidelberg},
 publisher = {{Springer Nature}},
 isbn = {978-3-540-32488-1},
 series = {Lecture Notes in Computer Science},
 editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and {van den Herik}, H. Jaap and Bj{\"o}rnsson, Yngvi and Netanyahu, Nathan S.},
 doi = {10.1007/11674399}
}


@proceedings{IEEEComputationalIntelligenceSociety.2010,
 year = {2010},
 title = {2010 IEEE Symposium on Computational Intelligence and Games (CIG 2010): Copenhagen, Denmark, 18 - 21 August 2010},
 address = {Piscataway NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-6295-7},
 institution = {{IEEE Computational Intelligence Society} and {IEEE Symposium on Computational Intelligence and Games} and {IEEE Conference on Computational Intelligence and Games} and CIG}
}


@proceedings{InstituteofElectricalandElectronicsEngineers.2012,
 year = {2012},
 title = {2012 IEEE Conference on Computational Intelligence and Games (CIG 2012): Granada, Spain, 11 - 14 September 2012},
 address = {Piscataway NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-1194-6},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Conference on Computational Intelligence and Games} and CIG}
}


@misc{Kang.2012,
 author = {Kang, Lukas},
 year = {2012},
 title = {Endgame Detection in Tron},
 file = {952af6d2-a6b7-4f65-89a1-ab54871427b0:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\952af6d2-a6b7-4f65-89a1-ab54871427b0.pdf:pdf}
}


@incollection{Knegt.2019,
 author = {Knegt, Stefan J. L. and Drugan, Madalina M. and Wiering, Marco A.},
 title = {Learning from Monte Carlo Rollouts with Opponent Models for Playing Tron},
 pages = {105--129},
 volume = {11352},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-05452-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {{van den Herik}, Jaap and Rocha, Ana Paula},
 booktitle = {Agents and Artificial Intelligence},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-05453-3{\textunderscore }6}
}


@article{Knuth.1975,
 author = {Knuth, Donald E. and Moore, Ronald W.},
 year = {1975},
 title = {An analysis of alpha-beta pruning},
 pages = {293--326},
 volume = {6},
 number = {4},
 issn = {0004-3702},
 journal = {Artificial Intelligence}
}


@article{Korf.1985,
 abstract = {The complexities of various search algorithms are considered in terms of time, space, and cost of solution path. It is known that breadth-first search requires too much space and depth-first search can use too much time and doesn't always find a cheapest path. A depth-first iterative-deepening algorithm is shown to be asymptotically optimal along all three dimensions for exponential tree searches. The algorithm has been used successfully in chess programs, has been effectively combined with bi-directional search, and has been applied to best-first heuristic search as well. This heuristic depth-first iterative-deepening algorithm is the only known algorithm that is capable of finding optimal solutions to randomly generated instances of the Fifteen Puzzle within practical resource limits.},
 author = {Korf, Richard E.},
 year = {1985},
 title = {Depth-first iterative-deepening},
 url = {http://www.sciencedirect.com/science/article/pii/0004370285900840},
 pages = {97--109},
 volume = {27},
 number = {1},
 issn = {0004-3702},
 journal = {Artificial Intelligence},
 doi = {10.1016/0004-3702(85)90084-0}
}


@book{Kranakis.2012,
 year = {2012},
 title = {Fun with algorithms: 6th international conference, FUN 2012, Venice, Italy, June 4 - 6, 2012 ; proceedings},
 address = {Berlin},
 volume = {7288},
 publisher = {Springer},
 isbn = {978-3-642-30346-3},
 series = {Lecture Notes in Computer Science},
 editor = {Kranakis, Evangelos},
 doi = {10.1007/978-3-642-30347-0}
}


@article{Lanctot.2013,
 author = {Lanctot, Marc and Teuling, Niek and Winands, Mark},
 year = {2013},
 title = {Monte Carlo Tree Search for Simultaneous Move Games: A Case Study in the Game of Tron},
 file = {ddc0f008-904c-4d88-ae83-4e26031a8bac:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\ddc0f008-904c-4d88-ae83-4e26031a8bac.pdf:pdf}
}


@article{Luke.2005,
 author = {Luke, Sean and Cioffi-Revilla, Claudio and Panait, Liviu and Sullivan, Keith and Balan, Gabriel},
 year = {2005},
 title = {MASON:~A Multiagent Simulation Environment},
 pages = {517--527},
 issn = {0037-5497},
 journal = {SIMULATION},
 file = {c75b6fdd-96c4-4cb0-9e84-ac4fea1103f6:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\c75b6fdd-96c4-4cb0-9e84-ac4fea1103f6.pdf:pdf}
}


@inproceedings{Macal.2009,
 author = {Macal, Charles M. and North, Michael J.},
 title = {Agent-based modeling and simulation},
 pages = {86--98},
 publisher = {IEEE},
 isbn = {978-1-4244-5770-0},
 editor = {Rossetti, Manuel D.},
 booktitle = {Proceedings of the 2009 Winter Simulation Conference},
 year = {2009},
 address = {Piscataway, NJ},
 doi = {10.1109/WSC.2009.5429318},
 file = {http://ieeexplore.ieee.org/document/5429318/},
 file = {4ae6ca04-8acf-4338-ac80-9fed91ac0495:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\4ae6ca04-8acf-4338-ac80-9fed91ac0495.pdf:pdf}
}


@inproceedings{Masad.2015,
 author = {Masad, David and Kazil, Jacqueline},
 title = {Mesa: An Agent-Based Modeling Framework},
 pages = {51--58},
 publisher = {SciPy},
 series = {Proceedings of the Python in Science Conference},
 booktitle = {Proceedings of the 14th Python in Science Conference},
 year = {2015},
 doi = {10.25080/Majora-7b98e3ed-009},
 file = {e77b453b-6ec9-43f6-8233-bcf30484817b:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\e77b453b-6ec9-43f6-8233-bcf30484817b.pdf:pdf}
}


@incollection{Miltzow.2012,
 author = {Miltzow, Tillmann},
 title = {Tron, a Combinatorial Game on Abstract Graphs},
 pages = {293--304},
 volume = {7288},
 publisher = {Springer},
 isbn = {978-3-642-30346-3},
 series = {Lecture Notes in Computer Science},
 editor = {Kranakis, Evangelos},
 booktitle = {Fun with algorithms},
 year = {2012},
 address = {Berlin},
 doi = {10.1007/978-3-642-30347-0{\textunderscore }29},
 file = {8587f7e0-63cb-4dba-b2b7-3ab8d668ce92:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\8587f7e0-63cb-4dba-b2b7-3ab8d668ce92.pdf:pdf}
}


@misc{Mnih.19.12.2013,
 abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
 date = {19.12.2013},
 title = {Playing Atari with Deep Reinforcement Learning},
 url = {http://arxiv.org/pdf/1312.5602v1},
 keywords = {Computer Science - Learning},
 file = {http://arxiv.org/abs/1312.5602v1},
 file = {https://arxiv.org/pdf/1312.5602v1.pdf},
 file = {1312.5602.pdf}
}


@article{Mnih.2015,
 abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
 year = {2015},
 title = {Human-level control through deep reinforcement learning},
 pages = {529--533},
 volume = {518},
 number = {7540},
 journal = {Nature},
 doi = {10.1038/nature14236},
 file = {481e24dd-b61e-453f-9382-3078bb5d07c0:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\481e24dd-b61e-453f-9382-3078bb5d07c0.pdf:pdf}
}


@article{North.2013,
 author = {North, Michael J. and Collier, Nicholson T. and Ozik, Jonathan and Tatara, Eric R. and Macal, Charles M. and Bragen, Mark and Sydelko, Pam},
 year = {2013},
 title = {Complex adaptive systems modeling with Repast Simphony},
 pages = {1035},
 volume = {1},
 number = {1},
 journal = {Complex Adaptive Systems Modeling},
 doi = {10.1186/2194-3206-1-3},
 file = {157c2809-c1ce-4a32-b77a-a87792e3defa:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\157c2809-c1ce-4a32-b77a-a87792e3defa.pdf:pdf}
}


@misc{OpenAI.2019,
 abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
 author = {OpenAI and : and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and {Pinto, Henrique Pond{\'e} de Oliveira} and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
 date = {2019},
 title = {Dota 2 with Large Scale Deep Reinforcement Learning},
 url = {https://arxiv.org/pdf/1912.06680},
 keywords = {Machine Learning (cs.LG);Machine Learning (stat.ML)},
 file = {OpenAI,  et al. 2019 - Dota 2 with Large Scale.pdf},
 file = {https://arxiv.org/pdf/1912.06680.pdf}
}


@inproceedings{Padmanabhan.2019,
 author = {Padmanabhan, Jayashree and {Bala P.}, Geetha and Rajkumar, S.},
 title = {Learning based Approximation Algorithm: A Case Study in Learning through Gaming},
 pages = {404--408},
 publisher = {IEEE},
 isbn = {978-1-7281-5286-8},
 booktitle = {11th International Conference on Advanced Computing},
 year = {2019},
 address = {[Piscataway, NJ]},
 doi = {10.1109/ICoAC48765.2019.246876},
 file = {6648fb59-51ab-403f-819b-2ea570b90d2a:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\6648fb59-51ab-403f-819b-2ea570b90d2a.pdf:pdf}
}


@incollection{Perez.2019,
 author = {Perez, Nicolas and Oommen, B. John},
 title = {Multi-Minimax: A New AI Paradigm for Simultaneously-Played Multi-player Games},
 pages = {41--53},
 volume = {11919},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-35287-5},
 series = {Lecture Notes in Computer Science},
 editor = {Birukou and Liu},
 booktitle = {AI 2019: Advances in Artificial Intelligence},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-35288-2{\textunderscore }4},
 file = {b88321c0-063b-46b7-8848-accc8d422597:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\b88321c0-063b-46b7-8848-accc8d422597.pdf:pdf}
}


@inproceedings{Perez.2019b,
 author = {Perez, Nicolas and Oommen, B. John},
 title = {Multi-Minimax: A New AI Paradigm for Simultaneously-Played Multi-player Games},
 pages = {41--53},
 booktitle = {Australasian Joint Conference on Artificial Intelligence},
 year = {2019}
}


@inproceedings{Perick.2012,
 author = {Perick, Pierre and St-Pierre, David L. and Maes, Francis and Ernst, Damien},
 title = {Comparison of different selection strategies in Monte-Carlo Tree Search for the game of Tron},
 pages = {242--249},
 publisher = {IEEE},
 isbn = {978-1-4673-1194-6},
 booktitle = {2012 IEEE Conference on Computational Intelligence and Games (CIG 2012)},
 year = {2012},
 address = {Piscataway NJ},
 doi = {10.1109/CIG.2012.6374162},
 file = {http://ieeexplore.ieee.org/document/6374162/},
 file = {8266c777-a721-4500-81de-ce487bc5b859:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\8266c777-a721-4500-81de-ce487bc5b859.pdf:pdf}
}


@article{Raja.2012,
 author = {Raja, P. and Pugazhenthi, S.},
 year = {2012},
 title = {Optimal path planning of mobile robots: A review},
 volume = {7},
 journal = {International Journal of the Physical Sciences},
 doi = {10.5897/IJPS11.1745}
}


@proceedings{Rossetti.2009,
 year = {2009},
 title = {Proceedings of the 2009 Winter Simulation Conference: December 13 - 16, 2009, Austin, Texas, U.S.A. ; inclusion of MASM (Modeling and analysis of semiconductor manufacturing)},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-5770-0},
 editor = {Rossetti, Manuel D.},
 institution = {{Institute of Electrical and Electronics Engineers} and {Winter Simulation Conference} and WSC and {MASM (Modeling and analysis of semiconductor manufacturing)}}
}


@inproceedings{S.Karaman.2011,
 author = {{S. Karaman} and {M. R. Walter} and {A. Perez} and {E. Frazzoli} and {S. Teller}},
 title = {Anytime Motion Planning using the RRT*},
 pages = {1478--1483},
 booktitle = {2011 IEEE International Conference on Robotics and Automation},
 year = {2011},
 doi = {10.1109/ICRA.2011.5980479}
}


@article{Saffidine.,
 abstract = {AAAI Publications},
 author = {Saffidine, Abdallah and Finnsson, Hilmar and Buro, Michael},
 title = {Alpha-Beta Pruning for Games with Simultaneous Moves},
 file = {5971da45-2d05-41ca-afc2-d77902763484:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\5971da45-2d05-41ca-afc2-d77902763484.pdf:pdf}
}


@inproceedings{Samothrakis.2010,
 author = {Samothrakis, Spyridon and Rob, David and Lucas, Simon M.},
 title = {A UCT agent for Tron: Initial investigations},
 pages = {365--371},
 publisher = {IEEE},
 isbn = {978-1-4244-6295-7},
 booktitle = {2010 IEEE Symposium on Computational Intelligence and Games (CIG 2010)},
 year = {2010},
 address = {Piscataway NJ},
 doi = {10.1109/ITW.2010.5593331},
 file = {http://ieeexplore.ieee.org/document/5593331/},
 file = {eb989f6f-8c8a-4029-ae8e-4dfc2c0c2de0:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\eb989f6f-8c8a-4029-ae8e-4dfc2c0c2de0.pdf:pdf}
}


@article{Schadd.2011,
 author = {Schadd, Maarten P. D. and Winands, Mark H. M.},
 year = {2011},
 title = {Best Reply Search for Multiplayer Games},
 pages = {57--66},
 volume = {3},
 number = {1},
 issn = {1943-068X},
 journal = {IEEE Transactions on Computational Intelligence and AI in Games},
 doi = {10.1109/tciaig.2011.2107323},
 file = {Schadd, Winands 2011 - Best Reply Search for Multiplayer.pdf}
}


@article{Shannon.1950,
 author = {Shannon, Claude E.},
 year = {1950},
 title = {XXII. Programming a computer for playing chess},
 pages = {256--275},
 volume = {41},
 number = {314},
 journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science}
}


@article{Silver.2017,
 abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
 author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
 year = {2017},
 title = {Mastering the game of Go without human knowledge},
 pages = {354--359},
 volume = {550},
 number = {7676},
 journal = {Nature},
 doi = {10.1038/nature24270},
 file = {4645f88b-5f4b-463e-b295-05defe1dc8da:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\4645f88b-5f4b-463e-b295-05defe1dc8da.pdf:pdf}
}


@article{Silver.2018,
 abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
 author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
 year = {2018},
 title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
 pages = {1140--1144},
 volume = {362},
 number = {6419},
 journal = {Science (New York, N.Y.)},
 doi = {10.1126/science.aar6404},
 file = {12d3a527-7ac4-4a82-9180-4697317675b1:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\12d3a527-7ac4-4a82-9180-4697317675b1.pdf:pdf;d21cb35c-64a6-442d-b64e-62c67db805ac:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\d21cb35c-64a6-442d-b64e-62c67db805ac.pdf:pdf}
}


@article{Silver.2018b,
 abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
 author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
 year = {2018},
 title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
 pages = {1140--1144},
 volume = {362},
 number = {6419},
 journal = {Science (New York, N.Y.)},
 doi = {10.1126/science.aar6404},
 file = {alphazero_preprint.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30523106}
}


@proceedings{Springer.2019,
 year = {2019},
 title = {Australasian Joint Conference on Artificial Intelligence},
 institution = {Springer}
}


@incollection{Sturtevant.2006,
 author = {Sturtevant, Nathan},
 title = {Current Challenges in Multi-player Game Search},
 pages = {285--300},
 volume = {3846},
 publisher = {{Springer Nature}},
 isbn = {978-3-540-32488-1},
 series = {Lecture Notes in Computer Science},
 booktitle = {Computers and Games},
 year = {2006},
 address = {Berlin, Heidelberg},
 doi = {10.1007/11674399{\textunderscore }20},
 file = {Sturtevant 2006 - Current Challenges in Multi-player Game.pdf}
}


@book{Sutton.1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 year = {1998},
 title = {Reinforcement learning: An introduction},
 price = {USD 40.00},
 address = {Cambridge, Mass.},
 edition = {[Nachdr.]},
 publisher = {{MIT Press}},
 isbn = {9780262193986},
 series = {A Bradford book},
 file = {SuttonBartoIPRLBook2ndEd.pdf}
}


@article{Teuling.2013,
 author = {Teuling, Niek and Winands, Mark},
 year = {2013},
 title = {Monte-Carlo Tree Search for the Simultaneous Move Game Tron},
 file = {a3c4ff21-c17f-42b9-8a7e-5c56b077b4df:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\a3c4ff21-c17f-42b9-8a7e-5c56b077b4df.pdf:pdf}
}


@book{vandenHerik.2019,
 abstract = {Agents -- Artificial Intelligence -- Multi-Agent Systems -- Distributed Problem Solving -- Bayesian Networks -- Soft Computing -- Neural Networks.-Natural Language Processing -- Prim



This book contains the revised and extended versions of selected papers from the 10th International Conference, ICAART 2018, held in Funchal, Madeira, Portugal, in January 2018. The 45 full papers together with 42 short papers and 26 Posters were carefully reviewed and selected from 161 initial submissions. The papers are organized in topics such as Agents, Artificial Intelligence, Semantic Web, Multi-Agent Systems, Distributed Problem Solving, Agent Communication and much more},
 year = {2019},
 title = {Agents and Artificial Intelligence: 10th International Conference, ICAART 2018, Funchal, Madeira, Portugal, January 16 - 18, 2018, Revised Selected Papers},
 address = {Cham},
 volume = {11352},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-05452-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {{van den Herik}, Jaap and Rocha, Ana Paula},
 doi = {10.1007/978-3-030-05453-3}
}


@article{Watkins.1992,
 author = {Watkins, Christopher J. C. H. and Dayan, Peter},
 year = {1992},
 title = {Q-learning},
 pages = {279--292},
 volume = {8},
 number = {3-4},
 issn = {0885-6125},
 journal = {Machine Learning},
 doi = {10.1007/BF00992698},
 file = {Watkins-Dayan1992_Article_Q-learning.pdf}
}


@article{Wilensky.2004,
 author = {Wilensky, Uri and Tisue, Seth},
 year = {2004},
 title = {NetLogo: Design and Implementation of a Multi-Agent Modeling Environment},
 file = {69208446-eb60-4387-82c0-ff11877a3f22:C\:\\Users\\maxde\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\o5lqy133k7ekbrv7fi07h08zgc5f35kupmnhgjaj9w7qz2yfd4v\\Citavi Attachments\\69208446-eb60-4387-82c0-ff11877a3f22.pdf:pdf}
}


@article{wikipedia_alpha_beta,
 url="https://en.wikipedia.org/wiki/Alpha\%E2\%80\%93beta_pruning",
 urldate = {2021-01-16}
}

@article{wikipedia_spieltheorie,
 url="https://de.wikipedia.org/wiki/Spieltheorie",
 urldate = {2021-01-16}
}


@INPROCEEDINGS{Garrido.2006,  author={S. {Garrido} and L. {Moreno} and M. {Abderrahim} and F. {Martin}},  booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},   title={Path Planning for Mobile Robot Navigation using Voronoi Diagram and Fast Marching},   year={2006},  volume={},  number={},  pages={2376-2381},  doi={10.1109/IROS.2006.282649}}